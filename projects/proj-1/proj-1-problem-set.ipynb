{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize, sent_tokenize\n",
      "from collections import defaultdict\n",
      "from matplotlib import pyplot as plt\n",
      "# Or you can use pylab\n",
      "import pylab as pl\n",
      "import numpy as np\n",
      "# In case the following plots can not be shown inline, please uncomment the following line (keep the \"%\")\n",
      "# %pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scorer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1. Data Processing #\n",
      "\n",
      "Your first step is to write code that can apply the following\n",
      "preprocessing steps. You will have to run this code fairly quickly on\n",
      "the test data when you receive it, so make sure it is modular and\n",
      "well-written.\n",
      "\n",
      "- You will edit a function that takes as its argument a \"key\" document.\n",
      "  It should produce a \"BOW\" (bag-of-words) document.\n",
      "  Each line of the key document contains a filename and a label.\n",
      "  Each line of the BOW document should contain a BOW representation of the corresponding\n",
      "  file in the key document. \n",
      "- A BOW representation looks like this: \"word:count word:count word:count...\" for every word that appears in\n",
      "  the document. Do not print words that have zero count. Use space delimiters.\n",
      "- Use NLTK's [tokenization package](http://nltk.org/api/nltk.tokenize.html) function \n",
      "  to divide each file into sentences, and each sentence into tokens.\n",
      "- Downcase all tokens\n",
      "- Only consider tokens that are completely alphabetic.\n",
      "\n",
      "I have provided some shell code, but you will have to fill in the tokenization and filtering steps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def docsToBOWs(keyfile):\n",
      "    with open(keyfile,'r') as keys:\n",
      "        with open(keyfile.replace('.key','.bow'),'w') as outfile:\n",
      "            for keyline in keys:\n",
      "                dataloc = keyline.rstrip().split(' ')[0]\n",
      "                fcounts = defaultdict(int) #hide this\n",
      "                with open(dataloc,'r') as infile:\n",
      "                    for line in infile: \n",
      "                        ## \n",
      "                        ## Write your own code here\n",
      "                        ## You can use NLTK tokenization package\n",
      "                        ##\n",
      "                for word,count in fcounts.items():\n",
      "                    print >>outfile,\"{}:{}\".format(word,count), #write the word and its count to a line\n",
      "                print >>outfile,\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, run these lines to produce the BOW files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docsToBOWs('train-imdb.key')\n",
      "docsToBOWs('dev-imdb.key')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next cell defines a [generator function](http://wiki.python.org/moin/Generators) that allows you to easily iterate through the dataset defined by a given keyfile. Each time you call next (possibly implicitly), it returns a [defaultdict](http://docs.python.org/2/library/collections.html#collections.defaultdict) containing words and counts for the next document in the sequence.\n",
      "You can see how this is used in the getAllCounts() function below, which takes a dataIterator as an argument. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dataIterator(keyfile):\n",
      "    with open(keyfile.replace('key','bow'),'r') as bows:\n",
      "        with open(keyfile,'r') as keys:\n",
      "            for keyline in keys:\n",
      "                textloc,label = keyline.rstrip().split(' ')\n",
      "                bowline = bows.readline()\n",
      "                kvs = bowline.split(' ')\n",
      "                fcounts = defaultdict(int)\n",
      "                for word,count in [x.split(':') for x in bowline.rstrip().split(' ')]:\n",
      "                    fcounts[word] = int(count)\n",
      "                yield fcounts,label"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataIterator above incrementally re-reads the keyfile and BOW file every time you call it. \n",
      "This is a good idea if you have huge data that won't fit in memory, but the file I/O involves some overhead.\n",
      "If you want, you can write a second dataIterator that iterates across data stored in memory, which\n",
      "will be faster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are the keyfiles that are relevant to this homework. \n",
      "at the beginning you won't have test-imdb.key"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainkey = 'train-imdb.key'\n",
      "devkey = 'dev-imdb.key'\n",
      "testkey = 'test-imdb.key' # You do not have this file now, refer to Deliverable 10 for more detail"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check 1**: How many unique words appear in the training set? (Types, not tokens.) I get 24861. Note how the dataIterator function is used here. fcounts is a defaultdict, which it returns for each document. We are currently ignoring the label, but that is also provided."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getAllCounts(datait):\n",
      "    allcounts = defaultdict(int)\n",
      "    for fcounts, label in datait:\n",
      "        for word, count in fcounts.items():\n",
      "            allcounts[word] += count\n",
      "    return allcounts\n",
      "\n",
      "ac_train = getAllCounts(dataIterator('train-imdb.key'))\n",
      "ac_dev = getAllCounts(dataIterator('dev-imdb.key'))\n",
      "print(len(ac_train.keys()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1**: Make a \"Zipf's law\" plot, with the log-rank (from 1 to the log of the total number of words) \n",
      "on the x-axis and the log count on the y-axis. \n",
      "Plot the training and dev data on the same axes with different colors. \n",
      "(And don't forget to label the axes!) \n",
      "Explain what you see in a text cell below the plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' TODO: your code here to draw a \"Zipf's law\" plot \n",
      "'''\n",
      "# In case you do not know how to use the \"plot()\" function\n",
      "# Here is a sample code\n",
      "x = np.array(range(0,5))\n",
      "y = x*x\n",
      "pl.plot(x,y)\n",
      "pl.xlabel('x')\n",
      "pl.ylabel('y')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(replace with your explanation of the plot above)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2. Word Lists #"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we will work with sentiment analysis based on word lists. The file \"sentiment-vocab.tff\" contains a sentiment lexicon from [ Wilson et al 2005](http://people.cs.pitt.edu/~wiebe/pubs/papers/emnlp05polarity.pdf). The code below reads the lexicon into memory, building sets of positive and negative words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' build word lists from file '''\n",
      "poswords = set()\n",
      "negwords = set()\n",
      "with open('sentiment-vocab.tff','r') as fin:\n",
      "    for line in fin:\n",
      "        stuff = dict()\n",
      "        for kvpair in line.rstrip().split(' '):\n",
      "            name, var = kvpair.partition(\"=\")[::2]\n",
      "            stuff[name] = var\n",
      "        if stuff['type'] == 'strongsubj':# or stuff['type'] == 'weaksubj':\n",
      "            if stuff['priorpolarity']=='negative':\n",
      "                negwords.add(stuff['word1'])\n",
      "            if stuff['priorpolarity']=='positive':\n",
      "                poswords.add(stuff['word1'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, you should write a classifier that classifies each instance in a testfile. The classification rule is:\n",
      "\n",
      "- 'POS' if the instance has more words from the positive list than the negative list\n",
      "- 'NEG' if the instance has more words from the negative list than the positive list\n",
      "- 'NEU' (neutral) if the instance has the same number of words from each list\n",
      "\n",
      "The classifier should write the output to a file. The function below provides a shell: it takes the right arguments, but classifies every document as positive."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def doWordListClassify(testfile,poswords,negwords,outfilename):\n",
      "    with open(outfilename,'w') as outfile:\n",
      "        for inst in dataIterator(testfile):\n",
      "            print >>outfile, 'POS' #other classes are 'NEG' and 'NEU'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below shows how to evaluate this classifier. Note that you get 41% accuracy just by classifying everything as positive. This is the \"most common class\" (MCC) baseline.\n",
      "\n",
      "The printed output is a confusion matrix. The rows indicate the key and the columns indicate the response. In this case, the response is always \"POS\", so there is only one column. The cell NEG/POS tells you how often an example that was labeled \"NEG\" in the key was labeled \"POS\" in the system response."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' You may want to read the \"scorer.py\" file \n",
      "    to see what functions it provides\n",
      "'''\n",
      "doWordListClassify(devkey,poswords,negwords,'list.out') #run the classifier on 'dev.key', outputting to 'list.out'\n",
      "confusion = scorer.getConfusion(devkey,'list.out') #compute the confusion matrix\n",
      "scorer.printScoreMessage(confusion) #print the confusion matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Write your word list classifier here. It should take the same arguments as doWordListClassify() above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' TODO: implement the doWordListClassify() function here '''\n",
      "def doWordListClassify(testfile,poswords,negwords,outfilename):\n",
      "    ## Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2a**: run your classifier on dev.key, and print the resulting confusion matrix.\n",
      "\n",
      "The confusion matrix should now have three columns, since the response should include every class at least once. The count of correct responses is found on the diagonal of the confusion matrix. What is the most frequent type of error?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' After you finish the doWordListClassify() function above,\n",
      "    You should be able to run this code and check the performance\n",
      "'''\n",
      "doWordListClassify(devkey,poswords,negwords,'list.out')\n",
      "confusion = scorer.getConfusion(devkey,'list.out')\n",
      "scorer.printScoreMessage(confusion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(include your explanation of the most frequent type of error here)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2b**: change your classifier by adding a threshold $k$. The new classification rule should be:\n",
      "\n",
      "- 'POS' if the number of positive words minus the number of negative words is greater than $k$\n",
      "- 'NEG' if the number of negative words minus the number of positive words is greater than $k$\n",
      "- 'NEU' otherwise\n",
      "\n",
      "Your new classifier should take a single additional argument, the threshold. \n",
      "\n",
      "Run the code below to try values of $k$ from 0 to 5, and plot the accuracy by $k$. Label your axes. \n",
      "\n",
      "Without testing it, can you predict what will happen to the accuracy as $k \\to \\infty$? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def doWordListClassify(testfile,poswords,negwords,outfilename,thresh):\n",
      "    ''' TODO: your code here'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' TODO: \n",
      "    1, Call the doWordListClassify() here\n",
      "    2, Draw the plot of accuracy vs. thresh\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(Explain what you think will happen as $k \\to \\infty$)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3. Naive Bayes #\n",
      "Build a Naive bayes classifier, using maximum a posteriori estimation of\n",
      "the parameters,\n",
      "$$\\theta_{ij} = P(w = j | y = i) = \\frac{\\text{count}(x=j,y=i) + \\alpha}{\\text{count}(y=i) + K\\alpha}$$\n",
      "where \n",
      "\n",
      "- $y=i$ indicates the class label $i$\n",
      "- $w=j$ indicates word $j$\n",
      "- $\\alpha$ is the smoothing parameter\n",
      "- $K$ is the total number of classes\n",
      "\n",
      "For each class, normalize by the sum of counts of words *in\n",
      "  that class*. In other words, $\\sum_j \\theta_{ij} = 1$ for each\n",
      "dictionary and for all $i$. You will probably need to work with log\n",
      "values of these parameters. Don't\n",
      "forget to also estimate the prior $P(y=i)$.\n",
      "\n",
      "Please write this yourself -- do not use other libraries, and try to do\n",
      "it without looking at other code online."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' TODO: '''\n",
      "import classifiers \n",
      "# \"Classifiers.py\" will be a file which includes all the classifiers used in the following, \n",
      "# such as Naive Bayes Classifier, Perceptron, etc.\n",
      "# You don't have this file now --- it is the one you should write by yourself"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labels = ('NEU','POS','NEG')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You will use this function to call your Naive Bayes classifier. You will need to support the following functions:\n",
      "\n",
      "- a constructor that takes the classes and $\\alpha$ as arguments\n",
      "- a trainAll() method that takes a dataIterator as arguments\n",
      "- a predict() method that takes a single instance (a defaultdict) as an argument, and returns two arguments:\n",
      "    + the top-scoring class\n",
      "    + a distribution over classes (which we are currently ignoring)\n",
      "- a getWeightsForLabel() method that returns a dictionary of log probabilities $p(w|y)$ for any given label\n",
      "\n",
      "I created an abstract LinearClassifier class that unifies a lot of this functionality across NaiveBayes and other classifiers. If you understood the lectures and readings, you should see that only the trainAll() and constructor methods need to be different among the linear classifiers that we have studied."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evalClassifier(classifier,outfilename,testfile=devkey):    \n",
      "    with open(outfilename,'w') as outfile:\n",
      "        for counts,label in dataIterator(testfile): #iterate through eval set\n",
      "            print >>outfile, classifier.predict(counts)[0] #print prediction to file\n",
      "    return scorer.getConfusion(testfile,outfilename) #run the scorer on the prediction file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Train the Naive Bayes classifier \n",
      "    and test it on dev data\n",
      "'''\n",
      "def doNB(alpha,outfilename,testfile=devkey):\n",
      "    nb = classifiers.NaiveBayes(labels,alpha) #initialize your naivebayes\n",
      "    nb.trainAll(dataIterator(trainkey)) #train it \n",
      "    return evalClassifier(nb,'nb.out',testfile) #evaluate it"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3**\n",
      "Train a classifier from the training data, and apply it to\n",
      "the development data, with $\\alpha = 0.1$. Report the confusion matrix and the accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Print the confusion matrix\n",
      "'''\n",
      "confusion = doNB(0.1,'nb.out')\n",
      "scorer.printScoreMessage(confusion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4** Try at least seven different values of $\\alpha$. Plot the accuracy on both the dev and training sets for each value, using [subplot](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot) to show two plots in the same cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' TODO: your code here for evaluating accuracy with different values of $\\alpha$ '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' TODO: your code here for draw the plot of accuracy with different values of $\\alpha$ '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(Use this cell to explain what you see in the plot above)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5**\n",
      "What are the words that are most predictive of positive versus negative text?\n",
      "You can measure this by $\\log \\theta_{+,i} - \\log \\theta_{-,i}$ (which is similar to the likelihood ratio test).\n",
      "Use $\\alpha = 1$ from the dev data.\n",
      "\n",
      "List the top five words and their counts for each class. Do the same for the top 5 words that predict negative versus positive.\n",
      "\n",
      "Consider using [operator.itemgetter()](http://docs.python.org/2.7/library/operator.html) for easily sorting dictionaries by their values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' TODO: implement the function here'''\n",
      "import operator\n",
      "\n",
      "def showWordsNaiveBayes(alpha):\n",
      "    ## Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Use the above function to show words'''\n",
      "sorted_diffs = showWordsNaiveBayes(1.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5b** Now do the same thing for $\\alpha = 100$. Which words look better to you? \n",
      "Which gave better accuracy? \n",
      "Explain what you think is going on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted_diffs = showWordsNaiveBayes(100.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(explain here)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5c** For $\\alpha=100$, how many of the top 100 words for positive sentiment are in the positive lexicon? \n",
      "How many are in the negative lexicon? Show them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' The example code for the positive lexicon '''\n",
      "poslex = [x[0] for x in sorted_diffs[-100:] if x[0] in poswords]\n",
      "print len(poslex),poslex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Your code here for the negative lexicon'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5d** How many of the top 100 words for negative sentiment are in the negative lexicon? How many are in the positive lexicon?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Your code here '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Your code here '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4. Perceptron #\n",
      "\n",
      "Implement a perceptron classifier. Using the feature-function\n",
      "representation, include features for each word-class pair, and also an\n",
      "``offset'' feature for each class. Given a set of word counts $\\vec{x}_i$,\n",
      "a true label $y_i$, and a guessed label $\\hat{y}$, your update will be\n",
      "\\begin{align*}\n",
      "\\hat{y} & \\leftarrow \\text{argmax}_y \\vec{\\theta}' f(\\vec{w}_i,y)\\\\\n",
      "\\vec{\\theta} & \\leftarrow \\vec{\\theta} + f(\\vec{w}_i, y_i) - f(\\vec{w}_i, \\hat{y}).\n",
      "\\end{align*}\n",
      "\n",
      "Please write this yourself -- do not use any libraries, and try not to look\n",
      "at other code online.\n",
      "\n",
      "Your perceptron class should also support *trainAll()*, *getWeightsForLabel()*, and *predict()*.\n",
      "In my code, I generalize a lot of the functionality between Perceptron and NaiveBayes,\n",
      "using a LinearClassifier base class. However, the constructor will be different because\n",
      "Perceptron does not have an $\\alpha$ parameter.\n",
      "\n",
      "**sanity** If you are not careful, learning can be slow. You may need to\n",
      "think a little about how to do this update efficiently. On my laptop,\n",
      "I can make 30 passes on the training data in roughly three minutes, including\n",
      "evaluating the accuracy on the dev and training sets."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 6** Using this update, make several training passes through the data\n",
      "(at least 20). Compute the accuracy on both the training and\n",
      "development sets after each pass. Plot them on a line graph, with the\n",
      "number of iterations on the x-axis, accuracy on the y-axis. Print the\n",
      "confusion matrix for the final iteration.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Initialization \n",
      "    \n",
      "    Your implementation of perceptron should be included in \"classifiers.py\" file\n",
      "'''\n",
      "per = classifiers.Perceptron(labels) #your perceptron class should take the list of labels as its only argument\n",
      "dv_accs = [] #store dev accuracy here\n",
      "tr_accs = [] #store training accuracy here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Running Perceptron '''\n",
      "for it in xrange(20):\n",
      "    per.trainAll(dataIterator(trainkey))\n",
      "    dv_accs.append(scorer.accuracy(evalClassifier(per,'per.out'))) #evaluate on the dev set\n",
      "    tr_accs.append(scorer.accuracy(evalClassifier(per,'per.out',testfile='train-imdb.key'))) #evaluate on the training set\n",
      "    print dv_accs[-1],"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Drawing plots '''\n",
      "subplot(1,2,1,xlabel='iteration',ylabel='train. accuracy')\n",
      "plot(tr_accs,'rx-')\n",
      "subplot(1,2,2,xlabel='iterator',ylabel='dev. accuracy')\n",
      "plot(dv_accs,'bx-')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 5. Averaged Perceptron #\n",
      "\n",
      "Notice how the performance of the perceptron was very unstable. Now you will try to improve it using averaging.\n",
      "\n",
      "Conceptually, the idea is to keep a running total of the weights, and then divide at the end, after $T$ updates:\n",
      "\n",
      "\\begin{align*}\n",
      "\\hat{y} & \\leftarrow \\text{argmax}_y \\theta' f(\\vec{w}_i,y)\\\\\n",
      "\\theta^t & \\leftarrow \\theta^{t-1} + f(\\vec{w}_i, y_i) - f(\\vec{w}_i, \\hat{y})\\\\\n",
      "\\overline{\\theta} & = \\frac{1}{T} \\theta^T\n",
      "\\end{align*}\n",
      "\n",
      "Then you can use $\\overline{\\theta}$ to make predictions on the test set.\n",
      "\n",
      "But in practice, this is very inefficient. You can't store the weights after every update -- it's much too big. But you don't want to compute a running sum either. The reason is that the weight vector will quickly become dense, and this would require $O(\\#F)$ operations at every update, where $\\#F$ is the number of features. This is much more work than the standard perceptron update, which only involves the features that are active in the current instance. In a bag-of-words model, each document will typically have only a small fraction of the total vocabulary, and we would like each update to be linear in the number of features active in the document, not the total number of features.\n",
      "\n",
      "An efficient solution was pointed out by [Daume 2006](http://hal3.name/docs/daume06thesis.pdf). \n",
      "Let $\\delta_t$ indicate the update at time $t$.\n",
      "Then, assuming $\\theta^0 = 0$, we have:\n",
      "\n",
      "\\begin{align*}\n",
      "\\theta^t = & \\theta^{t-1} + \\delta_t \\\\\n",
      "= & \\sum_{t' < t} \\delta_{t'}\n",
      "\\end{align*}\n",
      "\n",
      "We would like to compute the sum of the weight vectors,\n",
      "\\begin{align*}\n",
      "\\sum_t^T \\theta_t = & \\sum_t^T \\sum_{t' \\leq t} \\delta_{t'} = T \\delta_0 + (T-1) \\delta_1 + (T - 2) \\delta_2 + \\ldots + \\delta_T \\\\ \n",
      "= & \\sum_t^T (T - t) \\delta_t\\\\\n",
      "= & T \\sum_t^T \\delta_t - \\sum_t^T t \\delta_t \\\\\n",
      "= & T \\theta_t - \\sum_t^T t \\delta_t \\\\\n",
      "\\frac{1}{T} \\sum_t^T \\theta_t = & \\theta_T - \\frac{1}{T} \\sum_t^T t \\delta_t\n",
      "\\end{align*}\n",
      "\n",
      "This means we need to keep another running sum, $\\sum_t^T t \\delta_t$, the sum of scaled updates. \n",
      "To compute the average, we divide by the number of updates $T$ and subtract it from the current weight vector.\n",
      "\n",
      "In my implementation, my AvgPerceptron class has a function called getAveragedClassifier(), which performs the averaging operation. It returns a simple Perceptron, with the weights set according to the formula above.\n",
      "\n",
      "**Deliverable 7** Implement averaged perceptron, and produce the same plots and confusion matrix as in deliverable 6.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def doAvgPerceptron(ap,testfile=devkey):\n",
      "    per = ap.getAveragedClassifier()\n",
      "    confusion = evalClassifier(per,'ap.out',testfile=testfile)\n",
      "    return scorer.accuracy(confusion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Your implementation of averaged perceptron should be included in \"classifiers.py\" file '''\n",
      "ap = classifiers.AvgPerceptron(labels)\n",
      "dv_accs = []\n",
      "tr_accs = []\n",
      "for it in xrange(20):\n",
      "    ap.trainAll(dataIterator(trainkey))\n",
      "    dv_accs.append(doAvgPerceptron(ap))\n",
      "    tr_accs.append(doAvgPerceptron(ap,testfile='train-imdb.key'))\n",
      "    print dv_accs[-1],"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subplot(1,2,1,xlabel='iteration',ylabel='train. accuracy')\n",
      "plot(tr_accs,'rx-')\n",
      "subplot(1,2,2,xlabel='iterator',ylabel='dev. accuracy')\n",
      "plot(dv_accs,'bx-')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 8a** What are the words most indicative of positive text? \n",
      "Use the weights for the positive class from the averaged perceptron to answer this question. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_th = ap.getAveragedClassifier().getWeightsForLabel('POS') \n",
      "# Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 8b**. What are the words most indicative of negative text? \n",
      "Use the weights for the negative class from the averaged perceptron to answer this question."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "neg_th = ap.getAveragedClassifier().getWeightsForLabel('NEG')\n",
      "# Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 8c**. For each word, compute the difference between its positive and negative weights in the averaged perceptron.\n",
      "Call this $\\sigma_i = \\theta_{+,i} - \\theta_{-,i}$\n",
      "\n",
      "Compute $\\sigma$ for the averaged perceptron, as well as for the two Naive Bayes classifiers from deliverable 5,\n",
      "(with $\\alpha = 1$ and $\\alpha = 100$).\n",
      "\n",
      "Make a scatter plot of $\\sigma$ for each pair of classifiers (you should have three scatter plots)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nb1 = classifiers.NaiveBayes(labels,1.0)\n",
      "nb1.trainAll(dataIterator('train-imdb.key'))\n",
      "\n",
      "nb100 = classifiers.NaiveBayes(labels,100.0)\n",
      "nb100.trainAll(dataIterator('train-imdb.key'))\n",
      "\n",
      "per = ap.getAveragedClassifier()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allwords = [x[0] for x,w in nb1.getWeightsForLabel('POS').items()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Implement the getWeights() function, which will be used in the next code section '''\n",
      "def getWeights(clf,words,label):\n",
      "    # You code here\n",
      "    # You can use the following code to help you understand what should be \n",
      "    # implemented here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def makeScatter(clf1,clf2):\n",
      "    allwords = [x[0] for x,w in clf1.getWeightsForLabel('POS').items()]\n",
      "    diff1 = getWeights(clf1,allwords,'POS') - getWeights(clf1,allwords,'NEG')\n",
      "    diff2 = getWeights(clf2,allwords,'POS') - getWeights(clf2,allwords,'NEG')\n",
      "    scatter(diff1,diff2)\n",
      "    xlabel('classifier 1')\n",
      "    ylabel('classifier 2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' The first plot '''\n",
      "makeScatter(nb1,nb100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' The second plot '''\n",
      "makeScatter(nb1,per)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' The third plot '''\n",
      "makeScatter(nb100,per)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 6. 7650 only: Making it better #\n",
      "\n",
      "Try to improve one of these classifiers. Here are a few ideas:\n",
      "\n",
      "- Implement the MIRA algorithm described in the reading\n",
      "  notes. This controls the step size of the perceptron update, making\n",
      "  bigger steps for instances that are \"more\" wrong.\n",
      "- Use bigram features. This is simpler for the perceptron, but\n",
      "  with a little thought it can be applied to Naive Bayes too.\n",
      "- Use feature hashing to make the perceptron much faster and more\n",
      "  memory efficient. See [this paper](http://alex.smola.org/papers/2009/Weinbergeretal09.pdf).\n",
      "- [This paper](http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf) has several ideas for improving Naive Bayes, try one.\n",
      "- Add special features for matching elements in the sentiment vocabulary. You could add features for each combination \n",
      "  of polarity and subjectivity, e.g. \"negative/weaksubj\" or \"positive/strongsubj.\"\n",
      "- Do more complicated preprocessing, such as stemming and stopword removal.\n",
      "\n",
      "**Deliverable 9** Clearly explain what you did, and why you thought it would\n",
      "work. Do an experiment to test whether it works. Creativity\n",
      "and thoughtfulness counts more than raw performance here.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(explain here)*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your implementation could also be included in the \"classifiers.py\" file\n",
      "# Then, call functions here to produce results and make a comparison with \n",
      "# your previous implementation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 7. Bakeoff! #"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "48 hours before the assignment is due, I will send you unlabeled test\n",
      "data. Your job is to produce a response file that I can evaluate. I'll \n",
      "present the results in class and give the best scorers a chance to explain\n",
      "what they did.\n",
      "\n",
      "** Deliverable 10 ** Run your best system from any part of the\n",
      "assignment on the test data. Recall that evalClassifier() produces a response file. Rename this file \n",
      "***lastname-firstname.response***, and include it in your submission on T-Square. (Please get this \n",
      "filename right, otherwise we may miss your submission to the bakeoff.) The top scores will be announced in\n",
      "class.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}