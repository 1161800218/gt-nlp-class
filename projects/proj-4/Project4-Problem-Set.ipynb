{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Project 4. Latent Semantic Analysis and Semi-Supervised Learning #\n",
      "\n",
      "In this assignment, you will use singular value decomposition (SVD) to learn about lexical semantics. You will have to work with \"big\" data\n",
      "- big enough that you will have to think carefully about speed and memory. Of particular importance will **sparse** matrix\n",
      "representations of your data. Some particular functions and classes you might need:\n",
      "\n",
      "- [scipy.sparse.csr_matrix](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) - matrix in compressed sparse row format\n",
      "- [scipy.sparse.diags](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.diags.html) - method for creating sparse diagonal matrices\n",
      "- [diagonal()](http://docs.scipy.org/doc/numpy/reference/generated/numpy.diagonal.html) - get the diagonal of a matrix\n",
      "- [sklearn.preprocessing.normalize](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) - efficiently normalize sparse matrices\n",
      "- [scipy.sparse.csr_matrix.asfptype()](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.asfptype.html) - upcast matrix to a floating point format\n",
      "- [scipy.cluster.vq.kmeans2](http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.vq.kmeans2.html#scipy.cluster.vq.kmeans2) - Classify a set of observations into k clusters using the k-means algorithm\n",
      "- [numpy.argsort](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html) - returns the indices that would sort an array\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "from scipy.sparse.linalg import svds\n",
      "from scipy.sparse import hstack, diags\n",
      "from sklearn.preprocessing import normalize\n",
      "\n",
      "import numpy as np\n",
      "import proj4_starter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. Loading the Data ##\n",
      "\n",
      "Call **C=proj4_starter.csv2csr('doc_trips.csv')** to load a sparse matrix $C$ of a word-document counts. The cell $c[i,j]$ should \n",
      "hold the count of word $i$ in document $j$.\n",
      "\n",
      "Call **idx, iidx=proj4_starter.readVocab('vocab.10k')** to load the vocabulary. You get two **dict** objects, mapping between words \n",
      "and indices in the matrix $C$. In **C[iidx['Obama'],:]**, you have the document counts for the word *Obama*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Load data and dictionaries\n",
      "'''\n",
      "C = proj4_starter.csv2csr('doc_trips.csv')\n",
      "idx, iidx = proj4_starter.readVocab('vocab.10k')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. Cosine Similarity ##\n",
      "\n",
      "The *cosine similarity* of two vectors $u$ and $v$ is defined as \n",
      "$$\\frac{\\sum_{i}u_iv_i}{\\sqrt{\\sum_iu_i^2\\sum_iv_i^2}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 1** Consider the words *coffee, play, crazy, facebook*, and *hermana* (Spanish for *sister*). For each of them, find \n",
      "the 10 most similar words according to cosine similarity of the rows in $C$.\n",
      "\n",
      "**Hint** The size of the vocabulary is nearly 10,000 words. You do not want to compute and store the entire $10K\\times 10K$ matrix \n",
      "of cosine similarities. Rather, you want to compute them on demand for a given row of the matrix. You may also want to do some\n",
      "precomputation to take care of denominator in advance. Whatever you do, don't lose the sparsity of $C$, or you will not be able \n",
      "to store it.\n",
      "\n",
      "**Sanity check** For *Obama*, the top 5 words I get are *Obama, President, Michelle, president, Egypt*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here is the word list\n",
      "word_list = ['coffee','play','crazy','facebook','hermana']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalizeRow(x):\n",
      "    '''\n",
      "    Normalize each row of x\n",
      "    \n",
      "    Args:\n",
      "    x - data matrix\n",
      "    Return:\n",
      "    row-normalized x\n",
      "    '''\n",
      "    \n",
      "\n",
      "def computeCosSimPerWord(word_idx, x):\n",
      "    '''\n",
      "    For a given data matrix, compute cosine similarity between the word vocab[word_index] and all words \n",
      "    (including itself)\n",
      "    \n",
      "    Args:\n",
      "    word_idx - the index of one word\n",
      "    x - data matrix\n",
      "    Return:\n",
      "    A list of indices of similar words \n",
      "    '''\n",
      "    \n",
      "## \n",
      "normalizedC = normalizeRow(C)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def printSimilarWords(x, word_list, sim_func, vocab=idx, ivocab=iidx):\n",
      "    '''\n",
      "    Print similar words for each word in \"word_list\". \n",
      "    The similarity between words are computed from data matrix \"x\"\n",
      "    The way of measuring similarity is defined iin \"sim_func\"\n",
      "    \n",
      "    You may need this function to help you understand the outputs of \"sim_func\"\n",
      "    '''\n",
      "    for word in word_list:\n",
      "        print word, ':', \n",
      "        word_idx = ivocab[word]\n",
      "        sim_idx = sim_func(word_idx, x)\n",
      "        for word2_idx in sim_idx:\n",
      "            print vocab[word2_idx],\n",
      "        print ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print the results\n",
      "'''\n",
      "printSimilarWords(normalizedC, word_list, sim_func=computeCosSimPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 2 ** Come up with five words of your own that you think might be interesting, and list the top 10 most similar \n",
      "for each. Try to choose a few different types of words, such as verbs, adjectives, names, emotions, abbreviations, or \n",
      "alternative spellings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Add five words to the following list\n",
      "'''\n",
      "additional_word_list = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print the results\n",
      "'''\n",
      "printSimilarWords(normalizedC, additional_word_list, sim_func=computeCosSimPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. Document Co-occurence ##\n",
      "\n",
      "Compute the document co-occurence matrix $D$, where $d_{i,j}$ is the probability $P(w_j|w_i)$ that word $j$ appears in a tweet, \n",
      "given that word $i$ appears. To do this, first compute the co-occurence counts $CC^\\top$. Substract the diagonal, then normalize \n",
      "each row. \n",
      "\n",
      "Note: it is possible to smooth this probability, but if you naively add some number to the matrix, you will lose sparsity \n",
      "and memory will blow up. You can do it unsmoothed."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 3** For each of the 10 examples above (my five words and your five words), find the 10 most similar words \n",
      "according to cosine similarity of the rows of $D$.\n",
      "\n",
      "**Sanity check** For *Obama*, the 5 words I get are *Obama, news, America, by, power*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_list = word_list + additional_word_list\n",
      "\n",
      "def computeCooccurMatrix(C):\n",
      "    '''\n",
      "    Compute the co-occurence matrix D from C\n",
      "    '''\n",
      "    \n",
      "\n",
      "D = computeCooccurMatrix(C)\n",
      "normalizedD = normalizeRow(D)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print the results\n",
      "'''\n",
      "printSimilarWords(normalizedD, word_list, sim_func=computeCosSimPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4. Latent Semantic Analysis ##\n",
      "\n",
      "Perform truncated SVD (**scipy.sparse.linalg.svds**) to obtain $\\mathbf{USV}^\\top\\approx \\mathbf{C}$ using $K=10$. \n",
      "Each row vector $\\mathbf{u}_i$\n",
      "is a description of the word $i$. You can compute similarity between pairs of words using the Euclidean norm \n",
      "$\\|\\mathbf{u}_i-\\mathbf{u}_j\\|_2$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 4(a)** For each of the 10 examples above, find the 10 most similar words according to Euclidean distance in $\\mathbf{U}$\n",
      "\n",
      "**Sanity check** For *Obama*, the top 5 words are *Obama, shxt, Walker, President, views*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def computeEuclidDistPerWord(word_idx, U):\n",
      "    '''\n",
      "    Compute the Euclid distance bwteen word vocab[word_idx] and all words \n",
      "    (including itself). Then, return the top 10 similar word indices\n",
      "    \n",
      "    Args:\n",
      "      word_idx - the word index\n",
      "      U - latent representation of words\n",
      "    Return:\n",
      "      Word indices of top 10 words similar to vocab[word_idx]\n",
      "    '''\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Once you finish the function computeEuclidDistPerWord, run the following code directly to print results\n",
      "'''\n",
      "Cfp= C.asfptype()\n",
      "U = svds(Cfp, 10)\n",
      "\n",
      "printSimilarWords(U, word_list, sim_func=computeEuclidDistPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 4(b) ** Now compute the same SVD with $K=50$, and again find the 10 most similar words according to Euclidean distance $U$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print results\n",
      "'''\n",
      "U = svds(Cfp, 50)\n",
      "\n",
      "printSimilarWords(U, word_list, sim_func=computeEuclidDistPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 5 ** Now compute the SVD of the matrix $\\mathbf{D}$, using with $K = 10$, and $K = 50$. Report \n",
      "the most similar words to each of the example words according to Euclidean distance in $U$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print results\n",
      "'''\n",
      "D= D.asfptype()\n",
      "U = svds(D, 10)\n",
      "\n",
      "printSimilarWords(U, word_list, sim_func=computeEuclidDistPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print results\n",
      "'''\n",
      "U = svds(Cfp, 50)\n",
      "\n",
      "printSimilarWords(U, word_list, sim_func=computeEuclidDistPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 6 ** Overall, which set of synonyms looks best to you? \n",
      "Count how many of the top 10 synonyms for each of the 5 example words \n",
      "(the ones I picked) have the same majority part of speech (e.g., *play* is a verb) as the cue word.\n",
      "Use the tagset from the [Twitter POS paper](http://www.cc.gatech.edu/~jeisenst/papers/acl2012pos.pdf)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(Your answer here)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 5. Local Context ##\n",
      "\n",
      "Local context captures the frequency with which words appear in each others\u2019 immediate context. We have provided a CSV file (succ_trips_50k.csv)  in which each line contains a triple $\\langle x,y,z\\rangle$, \n",
      "where $x$ and $y$ are term IDs and $z$ is the count of times where $y$ immediately follows $x$ . \n",
      "The vocabulary has now increased to 50K words. There is an associated vocabulary file, **vocab.50k**.\n",
      "\n",
      "Build a sparse matrix $\\mathbf{E}$ from these triples. Normalize the rows of $\\mathbf{E}$, such that $e_{i,j}=\\frac{n(i,j)}{n(i)}$, \n",
      "the probability of seeing word $j$ given that you have just seen word $i$. \n",
      "Mow form a matrix $\\mathbf{F} = [\\mathbf{E}~ \\mathbf{E}\u2019]$ by horizontally concatenating the normalized matrix $\\mathbf{E}$. \n",
      "You will perform sparse singular value decomposition on $\\mathbf{F}$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def constructF(E):\n",
      "    '''\n",
      "    Finish the following code to construct F from E\n",
      "    '''\n",
      "    ## Your code here\n",
      "    return F\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Load new dataset and the corresponding dictionaries\n",
      "'''\n",
      "idx_50, iidx_50 = proj4_starter.readVocab('vocab.50k')\n",
      "E = proj4_starter.csv2csr('succ_trips_50k.csv')\n",
      "F = constructF(E)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 7 ** For $K = 10$ and $K = 100$ compute the top 10 synonyms for each of your ten words. \n",
      "\n",
      "** Sanity check ** For *Obama*, I get : *Obama, Ronnie, Twidroyd, Eddie, order* from $K = 10$\n",
      "and *Obama, Ibaka, Rihanna, Sager, Kurt* from $K = 100$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to get the results with K=10\n",
      "'''\n",
      "U_f = svds(F, 10)\n",
      "printSimilarWords(U_f, word_list, sim_func=computeEuclidDistPerWord, vocab=idx_50, ivocab=iidx_50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to get the results with K=100\n",
      "'''\n",
      "U_f = svds(F,100)\n",
      "printSimilarWords(U_f, word_list, sim_func=computeEuclidDistPerWord, vocab=idx_50, ivocab=iidx_50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 8 ** Again, count the proportion of proposed synonyms that have the same POS as \n",
      "the cue words that I selected. Does local context or document context do better at matching the POS of the cue words? Why?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(Your answer here)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 6. Clustering ##\n",
      "\n",
      "Perform KMeans clustering on the rows of the latent semantic representation $US$ that you obtained in the previous section \n",
      "(recall that $U$ describes the words; $S$ scales the latent factors by importance).\n",
      "\n",
      "Let the number of clusters equal 200. You may use an existing KMeans clustering library, \n",
      "or you may write your own. I used **scipy.cluster.vq.kmeans2**, and manually set the number of iterations to 100.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.cluster.vq import kmeans2, kmeans\n",
      "\n",
      "n_clusters = 200\n",
      "'''\n",
      "Your code here for clustering \n",
      "'''\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 9(a)**  Make a histogram with 10 bins for the sizes of all the clusters. Consider the hist() function in [pyplot](http://matplotlib.org/api/pyplot_api.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Your code here to draw the histogram\n",
      "\n",
      "Please include the plot inline\n",
      "'''\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 9(b)** Out of your ten examples words (you can exclude *hermana*), pick the one in the smallest cluster. \n",
      "Examine the words in this cluster, and list three that you think belong, and three that you think do not belong."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(Your answer here)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 7. Semi-Supervised Learning ##\n",
      "\n",
      "There are two choices for this part of the assignment. For full credit, you can work on semisupervised learning \n",
      "for your Twitter POS tagger. But if you never got the tagger to work properly, you can get partial credit by \n",
      "working with the dependency parser. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 7.1  POS Tagger ###\n",
      "\n",
      "Add the word clusters as features in your Twitter POS tagger. Start with your best performing tagger from **Project 2** (based on the dev data). Add a feature for each cluster/tag pair, e.g. C174/N, C189/V, etc. You will then compute the accuracy with training sets of various sizes, comparing the performance of your model with and without the cluster features."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 11 (Option A)** Build training sets including the first 50, 100, 200, 500, and 1000 *sentences* (not words). \n",
      "Train your tagger on each training set, using your original features, and plot the accuracy on the development set. \n",
      "Then retrain you tagger, including the new word cluster features, and plot accuracy on the development set on the same plot. \n",
      "Run for at least 10 iterations in each case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Your code here\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You may want to try larger numbers of clusters to improve performance. \n",
      "\n",
      "You can even include the results from multiple clusterings with different numbers of clusters (50, 100, 200, 500, ...), \n",
      "using features like C43-50/N (cluster 43 of 50, with tag N), C377-500/V (cluster 377 of 500, with tag V).\n",
      "\n",
      "You can also use clusters for the features of neighboring words."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 7.2 Dependency Parser ###\n",
      "\n",
      "If you could not get your implementation of structured perceptron for Twitter POS tagging to work, \n",
      "and you don\u2019t want to keep trying, you can instead add distributional features to the dependency parser from homework 3.\n",
      "\n",
      "Download the Brown clusters from a [word representation website](http://metaoptimize.com/projects/wordreprs/), \n",
      "using the 1000-cluster model. Follow [this paper](http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf) on word representation, \n",
      "adding features for prefixes of length 4, 6, 10, and 20. \n",
      "The features should capture the prefix of the head and modifier. You may also try \u201chybrid\u201d \n",
      "features that combine a prefix for the head, and the POS tag of  the modifier - and vice versa. \n",
      "See [this paper](http://www.cs.columbia.edu/~mcollins/papers/koo08acl.pdf) for more ideas and details.\n",
      "\n",
      "**Deliverable 11 (Option B)** Building training sets including the first 100, 500, 1000, 3000, and 7596 **sentences** (not words).\n",
      "Train your parser on each training set, using your best features, and plot the dev set accuracy. Then retrain your parser, adding \n",
      "the new Brown cluster features. Plot dev set accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Your code here\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 8. 7650 only##\n",
      "\n",
      "Find a paper that uses distributional features (such as word clusters) to improve performance on a task other than \n",
      "part-of-speech tagging, named entity recognition, and dependency parsing.\n",
      "\n",
      "**Deliverable 13 (Option A)** Describe:\n",
      "\n",
      "- the unlabeled data that they use\n",
      "- how the distributional features are computed \n",
      "- what task they are solving\n",
      "- how much the distributional features improve performance (or not)\n",
      "\n",
      "**OR**\n",
      "\n",
      "**Deliverable 13 (Option B)** Replace SVD with non-negative matrix factorization, using a toolkit like [nimfa](). \n",
      "Report the most similar words in the local context cases, do the clustering, and try adding these new cluster features to your POS tagger."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(Your answer here)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**OR**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Your code here\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}